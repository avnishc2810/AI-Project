{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30eccba4",
   "metadata": {},
   "source": [
    "First we will import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8c5aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "#Biopython for Fasta file processing\n",
    "from Bio import SeqIO\n",
    "\n",
    "#Tensorflow and Keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, LSTM, Embedding, BatchNormalization, GlobalMaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Scikit-learn for preprocessing and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import Input\n",
    "\n",
    "# Utility Libraries\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b04719-80a9-4aeb-a40d-eb88ceb774e4",
   "metadata": {},
   "source": [
    "Following is the code for k-mer encoding which comes under preprocessing of data obtained from the fasta files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b64b23d4-8de0-4e72-90fa-2ba528c8facc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Chimpanzee.fa: list index out of range\n"
     ]
    }
   ],
   "source": [
    "NUCLEOTIDES = {'A', 'C', 'G', 'T'}\n",
    "\n",
    "def clean_sequence(seq):\n",
    "    \"\"\"Remove invalid characters from the DNA sequence.\"\"\"\n",
    "    return ''.join(nuc for nuc in seq if nuc in NUCLEOTIDES)\n",
    "\n",
    "def kmer_count(sequence, k):\n",
    "    \"\"\"Count k-mers in a given DNA sequence and return a NumPy array of counts.\"\"\"\n",
    "    sequence = clean_sequence(sequence)\n",
    "    n = len(sequence)\n",
    "    kmer_counts = np.zeros((4 ** k,), dtype=int)\n",
    "\n",
    "    # Generate k-mer indices using itertools.product only once\n",
    "    kmer_index = {''.join(p): idx for idx, p in enumerate(itertools.product('ACGT', repeat=k))}\n",
    "    \n",
    "    # Count k-mers\n",
    "    for i in range(n - k + 1):\n",
    "        kmer = sequence[i:i + k]\n",
    "        if kmer in kmer_index:\n",
    "            kmer_counts[kmer_index[kmer]] += 1\n",
    "            \n",
    "    return kmer_counts\n",
    "\n",
    "def encode_top_kmers(fasta_file, k, top_n=2):\n",
    "    \"\"\"Encode only the top N sequences from a FASTA file into k-mer counts.\"\"\"\n",
    "    kmer_features = []\n",
    "\n",
    "    for i, seq_record in enumerate(SeqIO.parse(fasta_file, \"fasta\")):\n",
    "        if i >= top_n:  \n",
    "            break\n",
    "        sequence = str(seq_record.seq)\n",
    "        kmer_counts = kmer_count(sequence, k)\n",
    "        kmer_features.append(kmer_counts)\n",
    "\n",
    "    return np.array(kmer_features)  # Return only kmer features\n",
    "\n",
    "def vectorize_kmers(kmer_features, all_possible_kmers):\n",
    "    \"\"\"\n",
    "    Convert k-mer feature counts into a DataFrame with all possible k-mers as columns.\n",
    "    Handles cases where kmer_features is a list of dictionaries or sparse input.\n",
    "    \"\"\"\n",
    "    # Convert kmer_features into a DataFrame\n",
    "    if isinstance(kmer_features, list) and isinstance(kmer_features[0], dict):\n",
    "        # Assume sparse input, convert to DataFrame\n",
    "        kmer_df = pd.DataFrame(kmer_features).fillna(0)\n",
    "        # Reorder columns to match all_possible_kmers\n",
    "        kmer_df = kmer_df.reindex(columns=all_possible_kmers, fill_value=0)\n",
    "    else:\n",
    "        # Assume dense input (2D array-like)\n",
    "        kmer_df = pd.DataFrame(kmer_features, columns=all_possible_kmers)\n",
    "    \n",
    "    return kmer_df\n",
    "\n",
    "# Function to append sequences 21-30 to the existing CSV file\n",
    "def append_kmer_features_to_csv(fasta_file, k, start_seq=21, end_seq=30, output_dir=\"kmer_features_csv\"):\n",
    "    # Generate all possible k-mers of length k\n",
    "    all_possible_kmers = [''.join(p) for p in itertools.product('ACGT', repeat=k)]\n",
    "    \n",
    "    # Process sequences 21-30 from the FASTA file\n",
    "    kmer_features = []\n",
    "    for i, seq_record in enumerate(SeqIO.parse(fasta_file, \"fasta\")):\n",
    "        if i < start_seq - 1:\n",
    "            continue\n",
    "        if i >= end_seq:\n",
    "            break\n",
    "        sequence = str(seq_record.seq)\n",
    "        kmer_counts = kmer_count(sequence, k)\n",
    "        kmer_features.append(kmer_counts)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    new_kmer_df = vectorize_kmers(kmer_features, all_possible_kmers)\n",
    "    \n",
    "    # Load existing CSV file\n",
    "    base_name = os.path.splitext(os.path.basename(fasta_file))[0]\n",
    "    output_csv = os.path.join(output_dir, f\"{base_name}_kmer_features.csv\")\n",
    "    if os.path.exists(output_csv):\n",
    "        existing_df = pd.read_csv(output_csv)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"CSV file {output_csv} does not exist.\")\n",
    "    \n",
    "    # Append new data\n",
    "    updated_df = pd.concat([existing_df, new_kmer_df], ignore_index=True)\n",
    "    \n",
    "    # Save back to the CSV\n",
    "    updated_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Appended sequences {start_seq}-{end_seq} for {fasta_file} to {output_csv}\")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "k = 4\n",
    "fasta_files = [\"Chimpanzee.fa\"]\n",
    "output_dir = \"kmer_features_csv\"\n",
    "\n",
    "# Process each FASTA file and append sequences 21-30\n",
    "for fasta_file in fasta_files:\n",
    "    try:\n",
    "        append_kmer_features_to_csv(fasta_file, k, start_seq=50001, end_seq=10000000, output_dir=output_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {fasta_file}: {e}\")\n",
    "\n",
    "\n",
    "# Generate all possible k-mers of length k\n",
    "# k = 4\n",
    "# all_possible_kmers = [''.join(p) for p in itertools.product('ACGT', repeat=k)]\n",
    "\n",
    "# Example usage with increased top_n\n",
    "# fasta_files = [\"Human.fa\", \"Chimpanzee.fa\", \"Mouse.fa\", \"FruitFly.fa\", \"Fungus.fa\"]\n",
    "# top_n = 20\n",
    "\n",
    "# # Directory to store the output CSV files\n",
    "# output_dir = \"kmer_features_csv\"\n",
    "# os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# # Process each FASTA file and save the features to CSV\n",
    "# for fasta_file in fasta_files:\n",
    "#     try:\n",
    "#         kmer_features = encode_top_kmers(fasta_file, k, top_n)\n",
    "#         vectorized_features = vectorize_kmers(kmer_features, all_possible_kmers)\n",
    "        \n",
    "#         # Construct output file name\n",
    "#         base_name = os.path.splitext(os.path.basename(fasta_file))[0]\n",
    "#         output_csv = os.path.join(output_dir, f\"{base_name}_kmer_features.csv\")\n",
    "        \n",
    "#         # Save the DataFrame to a CSV file\n",
    "#         vectorized_features.to_csv(output_csv, index=False)\n",
    "        \n",
    "#         print(f\"Vectorized features for {fasta_file} saved to {output_csv}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {fasta_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27317d45-1139-465e-96d4-a825c224ed10",
   "metadata": {},
   "source": [
    "Now, we will convert the kmer features into a consistent format, that is vectorize the kmers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c9aaa-ee41-416b-b22a-78ac500af988",
   "metadata": {},
   "source": [
    "The next step is to spilt the data for testing and training the model. The following code performs this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0bea08c2-20c2-4fbd-bd4d-7374f0c880dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homo sapiens: Loaded 706 rows from kmer_features_csv/Human_kmer_features.csv\n",
      "Pan troglodytes: Loaded 44449 rows from kmer_features_csv/Chimpanzee_kmer_features.csv\n",
      "Mus musculus: Loaded 61 rows from kmer_features_csv/Mouse_kmer_features.csv\n",
      "Drosophila melanogaster: Loaded 1870 rows from kmer_features_csv/FruitFly_kmer_features.csv\n",
      "Saccharomyces cerevisiae: Loaded 17 rows from kmer_features_csv/Fungus_kmer_features.csv\n",
      "Shape of vectorized_features: (47103, 256)\n",
      "Length of labels: 47103\n"
     ]
    }
   ],
   "source": [
    "# Species-to-label mapping\n",
    "species_labels = {\n",
    "    'Homo sapiens': 0,\n",
    "    'Pan troglodytes': 1,\n",
    "    'Mus musculus': 2,\n",
    "    'Drosophila melanogaster': 3,\n",
    "    'Saccharomyces cerevisiae': 4  \n",
    "}\n",
    "\n",
    "# Paths to precomputed CSV files\n",
    "csv_files = {\n",
    "    'Homo sapiens': \"kmer_features_csv/Human_kmer_features.csv\",\n",
    "    'Pan troglodytes': \"kmer_features_csv/Chimpanzee_kmer_features.csv\",\n",
    "    'Mus musculus': \"kmer_features_csv/Mouse_kmer_features.csv\",\n",
    "    'Drosophila melanogaster': \"kmer_features_csv/FruitFly_kmer_features.csv\",\n",
    "    'Saccharomyces cerevisiae': \"kmer_features_csv/Fungus_kmer_features.csv\"\n",
    "}\n",
    "\n",
    "# Combine features and labels\n",
    "vectorized_features = []\n",
    "labels = []\n",
    "\n",
    "for species, file_path in csv_files.items():\n",
    "    # Load k-mer features from CSV\n",
    "    species_features = pd.read_csv(file_path)\n",
    "    print(f\"{species}: Loaded {species_features.shape[0]} rows from {file_path}\")\n",
    "    \n",
    "    # Add features to the combined list\n",
    "    vectorized_features.append(species_features)\n",
    "    \n",
    "    # Generate corresponding labels\n",
    "    labels.extend([species_labels[species]] * len(species_features))\n",
    "\n",
    "# Concatenate all features into a single DataFrame\n",
    "vectorized_features = pd.concat(vectorized_features, ignore_index=True)\n",
    "\n",
    "# Sanity check: Features and labels must match\n",
    "print(\"Shape of vectorized_features:\", vectorized_features.shape)\n",
    "print(\"Length of labels:\", len(labels))\n",
    "\n",
    "if vectorized_features.shape[0] != len(labels):\n",
    "    raise ValueError(\"Mismatch between the number of features and labels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8946ea2-5e0e-47cb-b4a8-2b183d5da4b6",
   "metadata": {},
   "source": [
    "Truncating the sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d30f62ca-a8e0-45b6-b279-52f720820ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_and_truncate_sequences(fasta_file, max_length=10000):\n",
    "#     \"\"\"Cleans the sequence by removing 'N' and truncates it to a fixed maximum length.\"\"\"\n",
    "#     cleaned_sequences = []\n",
    "#     sequence_ids = []\n",
    "\n",
    "#     for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "#         sequence = str(seq_record.seq)\n",
    "#         # Clean the sequence by removing 'N'\n",
    "#         cleaned_seq = ''.join([nuc for nuc in sequence if nuc != 'N'])\n",
    "#         # Truncate the cleaned sequence\n",
    "#         truncated_seq = cleaned_seq[:max_length]\n",
    "#         cleaned_sequences.append(truncated_seq)\n",
    "#         sequence_ids.append(seq_record.id)\n",
    "\n",
    "#     return sequence_ids, cleaned_sequences\n",
    "\n",
    "# # Example usage\n",
    "# fasta_file = \"Human.fa\"  # Replace with the actual path\n",
    "# max_length = 10000  # Set truncation length\n",
    "# sequence_ids, cleaned_sequences = clean_and_truncate_sequences(fasta_file, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57608a-c827-4d15-917b-a9d2862d7a2b",
   "metadata": {},
   "source": [
    "CNN model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1bdb499-4896-4317-97a0-dcd163547aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (37682, 256, 1)\n",
      "Test data shape: (9421, 256, 1)\n",
      "Training labels shape: (37682, 5)\n",
      "Test labels shape: (9421, 5)\n",
      "Training labels distribution:\n",
      "1    35559\n",
      "3     1496\n",
      "0      565\n",
      "2       49\n",
      "4       13\n",
      "Name: count, dtype: int64\n",
      "Test labels distribution:\n",
      "1    8890\n",
      "3     374\n",
      "0     141\n",
      "2      12\n",
      "4       4\n",
      "Name: count, dtype: int64\n",
      "Epoch 1/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.2354 - loss: 1.6781 - val_accuracy: 0.9436 - val_loss: 1.5242\n",
      "Epoch 2/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.5084 - loss: 1.4546 - val_accuracy: 0.0404 - val_loss: 1.4736\n",
      "Epoch 3/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.4657 - loss: 1.4564 - val_accuracy: 0.0405 - val_loss: 1.5121\n",
      "Epoch 4/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.3498 - loss: 1.5212 - val_accuracy: 0.0404 - val_loss: 1.3315\n",
      "Epoch 5/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.3880 - loss: 1.4828 - val_accuracy: 0.0446 - val_loss: 1.4648\n",
      "Epoch 6/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4301 - loss: 1.6256 - val_accuracy: 0.9438 - val_loss: 1.2676\n",
      "Epoch 7/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.4774 - loss: 1.4975 - val_accuracy: 0.0407 - val_loss: 1.3859\n",
      "Epoch 8/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.3590 - loss: 1.3030 - val_accuracy: 0.9441 - val_loss: 1.3941\n",
      "Epoch 9/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.3812 - loss: 1.4975 - val_accuracy: 0.0011 - val_loss: 1.7525\n",
      "Epoch 10/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.3300 - loss: 1.3201 - val_accuracy: 9.5531e-04 - val_loss: 1.7643\n",
      "Epoch 11/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.2906 - loss: 1.3553 - val_accuracy: 0.0412 - val_loss: 1.3318\n",
      "Epoch 12/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.3660 - loss: 1.3066 - val_accuracy: 0.9446 - val_loss: 1.4556\n",
      "Epoch 13/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4394 - loss: 1.3551 - val_accuracy: 0.0036 - val_loss: 1.7414\n",
      "Epoch 14/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.3385 - loss: 1.2858 - val_accuracy: 0.0428 - val_loss: 1.2599\n",
      "Epoch 15/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.3792 - loss: 1.3567 - val_accuracy: 0.0135 - val_loss: 1.6661\n",
      "Epoch 16/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4519 - loss: 1.3934 - val_accuracy: 8.4917e-04 - val_loss: 1.9975\n",
      "Epoch 17/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.1089 - loss: 1.5299 - val_accuracy: 0.0433 - val_loss: 1.2358\n",
      "Epoch 18/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.2371 - loss: 1.5415 - val_accuracy: 0.9437 - val_loss: 1.1692\n",
      "Epoch 19/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.3410 - loss: 1.3895 - val_accuracy: 0.9440 - val_loss: 1.2473\n",
      "Epoch 20/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4568 - loss: 1.2985 - val_accuracy: 0.9449 - val_loss: 1.2782\n",
      "Epoch 21/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4769 - loss: 1.3126 - val_accuracy: 0.9441 - val_loss: 1.2875\n",
      "Epoch 22/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.3375 - loss: 1.4358 - val_accuracy: 0.9384 - val_loss: 1.3450\n",
      "Epoch 23/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.4739 - loss: 1.1708 - val_accuracy: 0.0483 - val_loss: 1.2275\n",
      "Epoch 24/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.4020 - loss: 1.2758 - val_accuracy: 8.4917e-04 - val_loss: 2.6679\n",
      "Epoch 25/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.2652 - loss: 1.3471 - val_accuracy: 0.9325 - val_loss: 1.3362\n",
      "Epoch 26/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.4261 - loss: 1.1831 - val_accuracy: 0.0121 - val_loss: 1.4418\n",
      "Epoch 27/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4281 - loss: 1.3026 - val_accuracy: 0.9450 - val_loss: 1.1409\n",
      "Epoch 28/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4054 - loss: 1.1770 - val_accuracy: 0.0478 - val_loss: 1.1034\n",
      "Epoch 29/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.3925 - loss: 1.0855 - val_accuracy: 0.0119 - val_loss: 1.5130\n",
      "Epoch 30/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4889 - loss: 1.2255 - val_accuracy: 0.7133 - val_loss: 1.3891\n",
      "Epoch 31/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.4302 - loss: 1.2322 - val_accuracy: 0.0489 - val_loss: 1.2897\n",
      "Epoch 32/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.3916 - loss: 1.2070 - val_accuracy: 0.9447 - val_loss: 1.0977\n",
      "Epoch 33/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.4826 - loss: 1.0897 - val_accuracy: 0.9403 - val_loss: 1.0936\n",
      "Epoch 34/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.4445 - loss: 1.1955 - val_accuracy: 0.9004 - val_loss: 1.3053\n",
      "Epoch 35/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4859 - loss: 1.1044 - val_accuracy: 0.0892 - val_loss: 1.3728\n",
      "Epoch 36/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.3176 - loss: 1.3199 - val_accuracy: 0.0111 - val_loss: 1.7587\n",
      "Epoch 37/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.4714 - loss: 1.0618 - val_accuracy: 0.8857 - val_loss: 1.3344\n",
      "Epoch 38/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.5487 - loss: 0.9940 - val_accuracy: 0.9012 - val_loss: 1.3045\n",
      "Epoch 39/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.5016 - loss: 1.0463 - val_accuracy: 0.9447 - val_loss: 1.0775\n",
      "Epoch 40/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.5412 - loss: 1.0485 - val_accuracy: 0.0032 - val_loss: 2.7099\n",
      "Epoch 41/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.4225 - loss: 1.0879 - val_accuracy: 0.0488 - val_loss: 1.1330\n",
      "Epoch 42/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.3951 - loss: 0.9862 - val_accuracy: 8.4917e-04 - val_loss: 3.8531\n",
      "Epoch 43/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.2804 - loss: 1.2397 - val_accuracy: 0.0123 - val_loss: 1.4476\n",
      "Epoch 44/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.4428 - loss: 1.0523 - val_accuracy: 0.0514 - val_loss: 0.9557\n",
      "Epoch 45/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4206 - loss: 0.9770 - val_accuracy: 0.9384 - val_loss: 1.1398\n",
      "Epoch 46/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.4149 - loss: 0.9788 - val_accuracy: 0.0037 - val_loss: 3.3470\n",
      "Epoch 47/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.3446 - loss: 1.2791 - val_accuracy: 0.0114 - val_loss: 2.0741\n",
      "Epoch 48/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.4070 - loss: 0.9058 - val_accuracy: 0.8760 - val_loss: 1.3276\n",
      "Epoch 49/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.5198 - loss: 1.0741 - val_accuracy: 0.9362 - val_loss: 1.2135\n",
      "Epoch 50/50\n",
      "\u001b[1m1178/1178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.3287 - loss: 1.4160 - val_accuracy: 0.9424 - val_loss: 1.0401\n",
      "\u001b[1m295/295\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.0499 - loss: 0.9575\n",
      "Test Accuracy: 5.14%\n"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(vectorized_features)\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Reshape to fit CNN (samples, time_steps, features)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "num_classes = len(np.unique(labels))  # Dynamically calculate number of classes\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Outputs for sanity check\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Training labels distribution:\\n{pd.Series(y_train.argmax(axis=1)).value_counts()}\")\n",
    "print(f\"Test labels distribution:\\n{pd.Series(y_test.argmax(axis=1)).value_counts()}\")\n",
    "\n",
    "#Define CNN model\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),  # Explicitly define the input shape\n",
    "        Conv1D(32, 4, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.2),\n",
    "        Conv1D(64, 4, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "cnn_model = create_cnn_model(input_shape, num_classes)\n",
    "# cnn_model.summary()\n",
    "# Add early stopping and checkpointing\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_cnn_model.keras', save_best_only=True, monitor='val_accuracy')\n",
    "\n",
    "# Compute class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50, batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    class_weight=class_weights_dict  # Include class weights\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# def create_optimized_cnn_model(input_shape, num_classes):\n",
    "#     model = Sequential([\n",
    "#         Input(shape=input_shape),  # Explicitly define the input shape\n",
    "        \n",
    "#         # First Conv Layer\n",
    "#         Conv1D(32, 4, activation='relu', kernel_regularizer=regularizers.l2(0.001)), \n",
    "#         MaxPooling1D(2),\n",
    "#         Dropout(0.1),\n",
    "        \n",
    "#         # Second Conv Layer\n",
    "#         Conv1D(64, 4, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "#         MaxPooling1D(2),\n",
    "#         Dropout(0.2),\n",
    "        \n",
    "#         # Flatten and Dense Layer\n",
    "#         Flatten(),\n",
    "#         Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "#         Dropout(0.4),\n",
    "        \n",
    "#         # Output Layer\n",
    "#         Dense(num_classes, activation='softmax')  \n",
    "#     ])\n",
    "    \n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=1e-4),  # Start with a smaller learning rate\n",
    "#         loss='categorical_crossentropy',\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# input_shape = (X_train.shape[1], 1)\n",
    "# cnn_model = create_optimized_cnn_model(input_shape, num_classes)\n",
    "# cnn_model.summary()\n",
    "\n",
    "# # Callbacks for early stopping and best model saving\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# checkpoint = ModelCheckpoint('best_cnn_model.keras', save_best_only=True, monitor='val_accuracy')\n",
    "\n",
    "# # Learning rate scheduler\n",
    "# lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# # Compute class weights\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "# class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# # Train the model\n",
    "# cnn_model.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=50, batch_size=32,  # You can try increasing epochs if needed\n",
    "#     validation_data=(X_test, y_test),\n",
    "#     callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    "#     class_weight=class_weights_dict  # Include class weights\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "# print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a824378-c577-45bc-8d33-d2599a76479b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
